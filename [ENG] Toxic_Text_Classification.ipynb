{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_Starter_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AkEoCKaHdpj"
      },
      "source": [
        "\n",
        "In this notebook, we will try the process of implementing RNN with Keras in order to classify text sentences.\n",
        "\n",
        "I.   **Firstly**, we'll import useful packages.\n",
        "\n",
        "II.   **Then**, we'll load the data and create a word embedding matrix using Glove.\n",
        "\n",
        "III.  **We'll try a simple RNN model** and then we will evaluate its performances.\n",
        "\n",
        "IV. Finally, we'll use techniques to increase our model's accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xY_w9I1cZni"
      },
      "source": [
        "**Task 1:** Setting Fre GPU in this Google Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4iAL0E0ciDS"
      },
      "source": [
        "## Mounting Google Drive locally\n",
        "**Task 2:** Mount the Google Driver into the Google Colab Driver.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8iz8Rp8H5pG"
      },
      "source": [
        "## TYPE YOUR CODE for task 2 here:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Text_Classification\n",
        "%ls"
      ],
      "metadata": {
        "id": "zN5AySsUzLiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeAakuO9cD5s"
      },
      "source": [
        "# I. Let import all useful packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "Wh3A_t_g0fy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWgEP6KSHmV_"
      },
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "import tensorflow.keras\n",
        "import datetime\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras.optimizers as Optimizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from sklearn.metrics import confusion_matrix as CM\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plot\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "import keras.models\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras_preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "import sklearn.metrics as metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvFTfBIscRwC"
      },
      "source": [
        "**Task 3**: Copy the dataset from Google Drive into Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_GxFMl7dFJ-"
      },
      "source": [
        "# II. Load the data.\n",
        "\n",
        "## About dataset.\n",
        "An invalid question is defined as a question intended to make a statement rather than look for helpful answers. Some characteristics that can signify that a question is invalid:\n",
        "\n",
        "* Has a non-neutral tone.\n",
        "* Is disparaging or inflammatory.\n",
        "* Isn't grounded in reality.\n",
        "* Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n",
        "\n",
        "The data includes the question that was asked, and whether it was identified as invalid (target = 1). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HhWwT-gpuN"
      },
      "source": [
        "**Task 4**: Load the dataset.\n",
        "* Load the data from CSV file.\n",
        "* Remove all the rows with NA values.\n",
        "* Split the data into 3 set: Training set, validation set and test set (0.9/0.05/0.05, random_seed = 9) with a same ratio of data number beween each class.\n",
        "* Print out these dataset's description.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing Imbalance Data\n",
        "#Load Data from Source\n",
        "'''\n",
        "Nhằm cải thiện độ chính xác mô hình, ngoài các yêu cầu cơ bản của mô hình \n",
        "ta thêm và một bước nữa đó là xử \n",
        "lí dữ liệu mất cân bằng\n",
        "'''\n",
        "data=pd.read_csv(\"train.csv\")\n",
        "input=data[\"question_text\"]\n",
        "label=data[\"target\"]"
      ],
      "metadata": {
        "id": "dlcVSlzgJ8eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.shape, label.shape"
      ],
      "metadata": {
        "id": "RdBWMC-RLfwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Imbalance Data\n",
        "label.value_counts() # Dữ liệu bị chênh lệch giữa 2 biến 1 và 0"
      ],
      "metadata": {
        "id": "_RRKjLFCKSQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=input.values.reshape(-1,1)\n",
        "label=label.values.reshape(-1,1)\n",
        "input.shape, label.shape"
      ],
      "metadata": {
        "id": "i6k47asqNGb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler(\n",
        "    sampling_strategy='majority',  # samples only the majority class\n",
        "    random_state=0,  # for reproducibility\n",
        "    replacement=True # if it should resample with replacement\n",
        ")  \n",
        "X_resampled, y_resampled = rus.fit_resample(input,label)"
      ],
      "metadata": {
        "id": "InzYYVp6LE06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print shape of output Data\n",
        "X_resampled.shape, y_resampled.shape"
      ],
      "metadata": {
        "id": "fY9qGIaDOQK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_process=pd.DataFrame()\n",
        "data_process[\"question_text\"]=X_resampled.reshape(-1,)\n",
        "data_process[\"target\"]=y_resampled"
      ],
      "metadata": {
        "id": "ly5XY_EXQo_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_process[\"question_text\"]=X_resampled.reshape(-1,)\n",
        "data_process[\"target\"]=y_resampled"
      ],
      "metadata": {
        "id": "WVrSGAsaQztk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9HMbZrqK1Rq"
      },
      "source": [
        "def load_data(data_link):\n",
        "    '''\n",
        "    input: data link.\n",
        "    output:\n",
        "        train_set, validation_set and test_set(0.95/0.05/0.05) without NA values.\n",
        "    '''\n",
        "    \n",
        "    ## TYPE YOUR CODE for task 4 here:\n",
        "    data=data_link\n",
        "    data = data.rename(columns={'target': 'label'})\n",
        "    train, val_test= train_test_split(data[[\"question_text\",\"label\"]],stratify=data[\"label\"] ,test_size = 0.05,  random_state = 0)\n",
        "    validation, test=train_test_split(val_test[[\"question_text\",\"label\"]],stratify=val_test[\"label\"] , test_size = 0.5,  random_state = 0)\n",
        "    return train, validation, test\n",
        "\n",
        "train_set, validation_set, test_set = load_data(data_process)\n",
        "print(train_set['label'].describe())\n",
        "print(validation_set['label'].describe())\n",
        "print(test_set['label'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIcOnRkbqofC"
      },
      "source": [
        "# Encoding text data.\n",
        "Let declare some fundamental parameters first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F3_zcCjHwzm"
      },
      "source": [
        "embed_size = 50 # how big is each word vector\n",
        "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 50 # max number of words in a question to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT8m71iixjxt"
      },
      "source": [
        "**Task 5:** Encode the dataset using Tokenizer and one-hot encoding vector.\n",
        "* Encode the text (question_text column) by turning each question text into a list of word indexes using [Tokenizer](https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do) with **max_features** and all the text sentences from the training and the validation set. \n",
        "* Turn each list of word indexes into an equal length - **max_len** (with truncation or padding as needed) using [pad_sequences](https://keras.io/preprocessing/sequence/).\n",
        "* Encode the label (label column) using [to_categorical](https://keras.io/utils/) function on Keras."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "pVSiB4igC9Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1MZKNs4xmfP"
      },
      "source": [
        "def encoding_textdata(train_set, validation_set, test_set, max_features, max_len):\n",
        "    '''\n",
        "    Input:\n",
        "    - Train/validation/test dataset.\n",
        "    - max_features, max_len.\n",
        "    Output:\n",
        "    - X train/validation/test, y train/validation/test.\n",
        "    - Tokenizer.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 5 here:\n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "    data=pd.concat([train_set, validation_set])\n",
        "    tokenizer.fit_on_texts(data[\"question_text\"])\n",
        "\n",
        "    train_token =tokenizer.texts_to_sequences(train_set[\"question_text\"])\n",
        "    val_token   =tokenizer.texts_to_sequences(validation_set[\"question_text\"])    \n",
        "    test_token  =tokenizer.texts_to_sequences(test_set[\"question_text\"])\n",
        "\n",
        "    X_tr=pad_sequences(train_token, maxlen= max_len)\n",
        "    X_va=pad_sequences(val_token, maxlen= max_len)   \n",
        "    # X_te=test_set[\"question_text\"] \n",
        "    X_te=pad_sequences(test_token, maxlen= max_len)  \n",
        "\n",
        "    y_tr,y_va,y_te=train_set[\"label\"], validation_set[\"label\"], test_set[\"label\"]\n",
        "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), tokenizer   \n",
        "\n",
        "(X_tr, y_tr), (X_va, y_va), (X_te, y_te), tokenizer = encoding_textdata(train_set, validation_set, test_set, max_features, max_len)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kpG-p30WUcc"
      },
      "source": [
        "**Task 6**: Create word embedding matrix.\n",
        "* Firstly, write a function to [load the GloVe dictionary.](https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db)\n",
        "* Then, create a word embedding matrix using GloVe dictionary with these parameters:\n",
        "    - Word embedding matrix shape: (Number of word, embed_size).\n",
        "    - Embed size: 50.\n",
        "    - Number of words: The minimum of (max_features, len(word_index)), while word_index is the dictionary of word which contains in tokenizer.\n",
        "    - If a word occurs in GloVe dictionary, we should take its initialization value as in GloVe dictionary. Otherwise, take a normal random value with mean and std as mean and std of GloVe dictionary value.\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47s8-SncWT3V"
      },
      "source": [
        "def get_coefs(word,*arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "def get_GloVe_dict(GloVe_link):\n",
        "    '''\n",
        "    input: GloVe link.\n",
        "    output: GloVe dictionary.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 6 here:\n",
        "    embeddings_index = {}\n",
        "    with open(GloVe_link) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]            \n",
        "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "            embeddings_index[word] = coefs\n",
        "    return  embeddings_index        \n",
        "GloVe_link = 'glove.6B.50d.txt'\n",
        "GloVe_dict = get_GloVe_dict(GloVe_link)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 50 # how big is each word vector\n",
        "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 50 # max number of words in a question to use"
      ],
      "metadata": {
        "id": "kOxIQSmWP4PA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min(max_features,line)"
      ],
      "metadata": {
        "id": "cERagLUHQ4nE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXRyFSLtr4_k"
      },
      "source": [
        "# Calculate mean and std of Glove\n",
        "values=[]\n",
        "data_value_of_golve=pd.DataFrame(columns=[\"values\"])   \n",
        "for i in GloVe_dict.values():\n",
        "    values.append(i)\n",
        "value=np.array(values)\n",
        "data_value_of_golve[\"values\"]=GloVe_dict.values()\n",
        "mean=data_value_of_golve[\"values\"].mean()\n",
        "array_std=np.array(data_value_of_golve[\"values\"].values)\n",
        "array_std=array_std.reshape(-1,50)\n",
        "std=array_std.std()\n",
        "\n",
        "def initialization_vector(mean,std):\n",
        "    \"\"\"\n",
        "    If a word occurs in GloVe dictionary, we should take its \n",
        "    initialization value as in GloVe dictionary.\n",
        "    Otherwise, take a normal random value with mean and std \n",
        "    as mean and std of GloVe dictionary value.\n",
        "    \"\"\"\n",
        "    initialization_vector = np.random.normal(loc=mean, scale=std, size=50)\n",
        "    return initialization_vector\n",
        "\n",
        "def create_embedding_matrix(GloVe_dict, tokenizer, max_features):\n",
        "    '''\n",
        "    input: GloVe dictionaray, tokenizer từ tập huấn luyện và tập kiểm định, số lượng đặc trưng tối đa.\n",
        "    output: Word embedding matrix.\n",
        "    '''\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embed_size))\n",
        "    num_words_in_embedding = 0\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = GloVe_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            num_words_in_embedding += 1\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            embedding_matrix[i]=initialization_vector(mean,std)\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(GloVe_dict, tokenizer, max_features)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWybjdQkqWrg"
      },
      "source": [
        "III. Modelling\n",
        "There are some steps we need to finish:\n",
        "Build the model.\n",
        "\n",
        "Compile the model.\n",
        "\n",
        "Train / fit the data to the model.\n",
        "\n",
        "Evaluate the model on the testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6AMfqQkqcET"
      },
      "source": [
        "## Build the model\n",
        "**Task 7:** We can build an easy model composed of different layers such as:\n",
        "* [Embedding](https://keras.io/layers/embeddings/) layer with max_features, embed_size and embedding_matrix.\n",
        "* [Bidirectional LSTM layer](https://keras.io/examples/nlp/bidirectional_lstm_imdb/?fbclid=IwAR3fEd6aWyeIDEhZSspjtCRiP0c0Jnz5-XdnUHQYwX8Tp8k9Ni4I8Q5tP9o) with number of hidden state = 50, dropout_rate = 0.1 and recurrent_dropout_rate = 0.1.\n",
        "* GlobalMaxPool1D.\n",
        "* Dense with number of unit = 50, activation = 'relu'.\n",
        "* Dropout with rate = 0.1.\n",
        "* Final dense with number of unit = number of class, activation = 'sigmoid'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7_eizWaqi_7"
      },
      "source": [
        "def create_model(max_len, max_features, embed_size):\n",
        "    '''\n",
        "    input: max_len, max_features, embed_size\n",
        "    output: model.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 7 here:\n",
        "    model = keras.Sequential()\n",
        "    embedding_layer= layers.Embedding(\n",
        "                          input_dim=embedding_matrix.shape[0], # thiết lập shape đầu vào\n",
        "                          output_dim=embed_size, # thiết lập kích thước của vectơ embedding\n",
        "                          embeddings_initializer=\"uniform\", # mặc định, khởi tại ngẫu nhiên\n",
        "                          weights=[embedding_matrix],\n",
        "                          input_length=max_len, # mỗi đầu vào dài bao nhiêu\n",
        "                          # trainable=False,\n",
        "                          name=\"embedding\") \n",
        "    \n",
        "    # text_vectorizer = TextVectorization(max_tokens=None, # có bao nhiêu từ trong từ vựng (toàn bộ các từ khác nhau trong text)\n",
        "    #                                 standardize=\"lower_and_strip_punctuation\", # cách xử lý text\n",
        "    #                                 split=\"whitespace\", # cách phân chia token\n",
        "    #                                 ngrams=None, # có tạo nhóm n-từ không?\n",
        "    #                                 output_mode=\"int\", # cách ánh xạ token thành số\n",
        "    #                                 output_sequence_length=None)\n",
        "    \n",
        "    model.add(layers.Input(shape=(50,)))\n",
        "    # model.add(text_vectorizer)\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Bidirectional(LSTM(50,return_sequences=True)))\n",
        "    model.add(layers.Dropout(0.1))\n",
        "    model.add(layers.GlobalMaxPool1D())\n",
        "    model.add(layers.Dense(50,activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.1))\n",
        "    model.add(layers.Dense(1,activation=\"sigmoid\"))   \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model(max_len, max_features, embed_size)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWcBKhzMux9Z"
      },
      "source": [
        "**Task 8:** Compile the model and setup the callback. Then print out the model summary.\n",
        "* [Compile](https://keras.io/models/model/#compile) the model with Adam Optimizaer, lr = 1e-2, suitable loss for binary classification problem and [\"F1-score\"](https://github.com/tensorflow/addons/issues/825) as metric.\n",
        "* Print out the model summary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_metric(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "AGYym43lqrsy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9l8EbG0ur1F"
      },
      "source": [
        "def optimize(model):\n",
        "    '''\n",
        "    Input: \n",
        "        Model.\n",
        "    Return: \n",
        "        Complied model.\n",
        "    '''\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss='binary_crossentropy', metrics=['acc',f1_metric])\n",
        "    return model\n",
        "\n",
        "model = optimize(model)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BlenccGzLVr"
      },
      "source": [
        "**Task 9**: Setup callback.\n",
        "* Create the [tensorboard callback](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) to save the logs.\n",
        "* Create the [checkpoint callback](https://machinelearningmastery.com/check-point-deep-learning-models-keras/) to save the checkpoint with the best accuracy after each epoch.\n",
        "* Create the [ReduceLROnPlateau](https://keras.io/callbacks/#reducelronplateau) callback with factor=0.3, patience=1 and \"Validation F1-score\" monitor.\n",
        "* Create the [early stopping callback](https://keras.io/callbacks/#earlystopping) with patience=7, mode = 'max' and \"Validation F1-score\" monitor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6x6dteutin0"
      },
      "source": [
        "def callback_model(checkpoint_name, logs_name):\n",
        "    '''\n",
        "    Input: \n",
        "        Checkpoint name, logs name tốt nhất.\n",
        "    Return: \n",
        "        Callback list có chứa tensorboard callback và checkpoint callback.\n",
        "    '''\n",
        "    #Tensorboard\n",
        "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    tensorboard_callback=tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "    tensorboard_callback=tf.keras.callbacks.TensorBoard(logs_name, histogram_freq=1)\n",
        "    #Early_Stopping\n",
        "    early_stopping = EarlyStopping(monitor = 'val_f1_metric',\n",
        "                               patience = 7,\n",
        "                               mode = 'max',\n",
        "                               restore_best_weights = True)\n",
        "\n",
        "    # Save the model with the minimum validation loss\n",
        "    checkpoint = ModelCheckpoint(checkpoint_name,monitor = 'val_f1_metric',verbose = 1,mode = 'max', save_best_only = True)\n",
        "\n",
        "    # reduce learning rate\n",
        "    reduce_lr = ReduceLROnPlateau(monitor = 'val_f1_metric',factor = 0.3,patience = 1)\n",
        "\n",
        "    callbacks_list=[tensorboard_callback,checkpoint,reduce_lr,early_stopping]                              \n",
        "    return callbacks_list\n",
        "\n",
        "checkpoint_name = 'weights.best.hdf5'\n",
        "logs_name = 'training_logs'\n",
        "callbacks_list = callback_model(checkpoint_name, logs_name)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nDCsHAC2HwW"
      },
      "source": [
        "**Task 10:** Train the model.\n",
        "\n",
        "* Train the model with 20 epochs with batch_size = 4096.\n",
        "* Return the model with best-checkpoint weights.\n",
        "\n",
        "*Hint*: Fit the model first, then reload the model (load_model function) with best-checkpoint weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xttwiHh4u0ES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386925dc-babe-4b5c-eeb0-c9236789df6a"
      },
      "source": [
        "def train_model(model, callbacks_list):\n",
        "    '''\n",
        "    Input: \n",
        "        Model and callback list,\n",
        "    Return: \n",
        "        Model with best-checkpoint weights.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 10 here:\n",
        "    model.fit(X_tr,\n",
        "              y_tr,              \n",
        "              epochs=20,\n",
        "              batch_size = 4096,\n",
        "              validation_data=(X_va,y_va),\n",
        "              callbacks=callbacks_list\n",
        "              )\n",
        "    return model\n",
        "\n",
        "model = train_model(model, callbacks_list)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.7929 - f1_metric: 0.7884\n",
            "Epoch 1: val_f1_metric improved from -inf to 0.89301, saving model to weights.best.hdf5\n",
            "38/38 [==============================] - 15s 170ms/step - loss: 0.4243 - acc: 0.7942 - f1_metric: 0.7912 - val_loss: 0.2845 - val_acc: 0.8901 - val_f1_metric: 0.8930 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9013 - f1_metric: 0.9023\n",
            "Epoch 2: val_f1_metric improved from 0.89301 to 0.90252, saving model to weights.best.hdf5\n",
            "38/38 [==============================] - 3s 83ms/step - loss: 0.2582 - acc: 0.9015 - f1_metric: 0.9026 - val_loss: 0.2665 - val_acc: 0.9005 - val_f1_metric: 0.9025 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9167 - f1_metric: 0.9175\n",
            "Epoch 3: val_f1_metric improved from 0.90252 to 0.90592, saving model to weights.best.hdf5\n",
            "38/38 [==============================] - 3s 82ms/step - loss: 0.2248 - acc: 0.9166 - f1_metric: 0.9174 - val_loss: 0.2642 - val_acc: 0.9040 - val_f1_metric: 0.9059 - lr: 0.0030\n",
            "Epoch 4/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9218 - f1_metric: 0.9227\n",
            "Epoch 4: val_f1_metric did not improve from 0.90592\n",
            "38/38 [==============================] - 3s 78ms/step - loss: 0.2144 - acc: 0.9218 - f1_metric: 0.9226 - val_loss: 0.2644 - val_acc: 0.9035 - val_f1_metric: 0.9051 - lr: 9.0000e-04\n",
            "Epoch 5/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9235 - f1_metric: 0.9242\n",
            "Epoch 5: val_f1_metric did not improve from 0.90592\n",
            "38/38 [==============================] - 3s 77ms/step - loss: 0.2112 - acc: 0.9235 - f1_metric: 0.9243 - val_loss: 0.2647 - val_acc: 0.9017 - val_f1_metric: 0.9032 - lr: 2.7000e-04\n",
            "Epoch 6/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9236 - f1_metric: 0.9244\n",
            "Epoch 6: val_f1_metric did not improve from 0.90592\n",
            "38/38 [==============================] - 3s 86ms/step - loss: 0.2100 - acc: 0.9237 - f1_metric: 0.9247 - val_loss: 0.2649 - val_acc: 0.9020 - val_f1_metric: 0.9033 - lr: 8.1000e-05\n",
            "Epoch 7/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9241 - f1_metric: 0.9249\n",
            "Epoch 7: val_f1_metric did not improve from 0.90592\n",
            "38/38 [==============================] - 3s 78ms/step - loss: 0.2093 - acc: 0.9242 - f1_metric: 0.9251 - val_loss: 0.2649 - val_acc: 0.9022 - val_f1_metric: 0.9036 - lr: 2.4300e-05\n",
            "Epoch 8/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9240 - f1_metric: 0.9248\n",
            "Epoch 8: val_f1_metric did not improve from 0.90592\n",
            "38/38 [==============================] - 3s 77ms/step - loss: 0.2096 - acc: 0.9239 - f1_metric: 0.9245 - val_loss: 0.2649 - val_acc: 0.9022 - val_f1_metric: 0.9036 - lr: 7.2900e-06\n",
            "Epoch 9/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9241 - f1_metric: 0.9248\n",
            "Epoch 9: val_f1_metric did not improve from 0.90592\n",
            "38/38 [==============================] - 3s 77ms/step - loss: 0.2098 - acc: 0.9242 - f1_metric: 0.9251 - val_loss: 0.2649 - val_acc: 0.9022 - val_f1_metric: 0.9036 - lr: 2.1870e-06\n",
            "Epoch 10/20\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9243 - f1_metric: 0.9250\n",
            "Epoch 10: val_f1_metric did not improve from 0.90592\n",
            "38/38 [==============================] - 3s 78ms/step - loss: 0.2092 - acc: 0.9242 - f1_metric: 0.9249 - val_loss: 0.2649 - val_acc: 0.9022 - val_f1_metric: 0.9036 - lr: 6.5610e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsG02Ao07Mc-"
      },
      "source": [
        "**Task 11:** Show the tensorboard in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard dev upload --logdir ./training_logs \\\n",
        "  --name \"Classification_Toxic_Comment\" \\\n",
        "  --description \"The Last Training\" \\\n",
        "  --one_shot"
      ],
      "metadata": {
        "id": "JdBLl2SbWotQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e993cc-eecb-498c-d1a6-91f7bb823f20"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "./training_logs\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=pIpCQ4nFUum4bF9XtCjTOOLhrrtZC2&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/1AdQt8qjtEZKctdaJB9si-WfAJ02FDDh0mtNqmWY4ObBfRz77s_K9nofcAM8\n",
            "\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/FWbr2VmjToGI7Z7QKcGJIg/\n",
            "\n",
            "\u001b[1m[2022-08-20T07:20:57]\u001b[0m Started scanning logdir.\n",
            "E0820 07:21:02.474343 140372331222912 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0820 07:21:05.166023 140372331222912 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0820 07:21:09.042366 140372331222912 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0820 07:21:12.301935 140372331222912 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0820 07:21:14.406944 140372331222912 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0820 07:21:24.168583 140372331222912 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0820 07:21:25.164394 140372331222912 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "\u001b[1m[2022-08-20T07:21:32]\u001b[0m Total uploaded: 873 scalars, 1611 tensors (1.1 MB), 1 binary objects (406.5 kB)\n",
            "\u001b[90mTotal skipped: 7 binary objects (4.5 MB)\n",
            "\u001b[0m\u001b[1m[2022-08-20T07:21:32]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/FWbr2VmjToGI7Z7QKcGJIg/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z1ed1CY8Rxh"
      },
      "source": [
        "**Task 12:** Prediction on test set.\n",
        "\n",
        "* Complete the get_prediction_classes function.\n",
        "* Print out the precision, recall and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHTjBLZYvx26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1443c2fc-f59e-4dbc-d419-caedbe2927a2"
      },
      "source": [
        "def get_prediction_classes(model, X, y):\n",
        "    ## TYPE YOUR CODE for task 13 here:\n",
        "    '''\n",
        "    Input: \n",
        "        Model and prediction dataset.\n",
        "    Return: \n",
        "        Prediction list and groundtrurth list with predicted classes.\n",
        "    '''\n",
        "    predictions=model.predict(X)\n",
        "    predictions=np.around(predictions,0)\n",
        "    groundtruths=y.values\n",
        "    groundtruths=groundtruths.reshape(-1,1)\n",
        "    return predictions, groundtruths \n",
        "\n",
        "test_predictions, test_groundtruths = get_prediction_classes(model,  X_te, y_te)\n",
        "print(precision_score(test_predictions, test_groundtruths))\n",
        "print(recall_score(test_predictions, test_groundtruths))\n",
        "print(f1_score(test_predictions, test_groundtruths))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9193069306930693\n",
            "0.8872431915910177\n",
            "0.9029905178701678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJwQahjp8hZs"
      },
      "source": [
        "**Task 13:** Perform the predicted result on test set using confusion matrix. Remember to show the class name in the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KKAmOGvv2Be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e1239593-e7fe-4255-9152-275d4da621d9"
      },
      "source": [
        "def plot_confusion_matrix(predictions, groundtruth, class_names):\n",
        "    ## TYPE YOUR CODE for task 13 here:    \n",
        "    cf=metrics.confusion_matrix(groundtruth,predictions)\n",
        "    ax= plot.subplot()\n",
        "    sns.heatmap(cf, annot=True, fmt='g', ax=ax)\n",
        "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "    ax.set_title('Confusion Matrix'); \n",
        "    ax.xaxis.set_ticklabels(class_names); ax.yaxis.set_ticklabels(class_names)\n",
        "class_names = ['valid', 'invalid']\n",
        "plot_confusion_matrix(test_predictions, test_groundtruths, class_names)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
